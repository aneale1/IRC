package tokenizer

import (
	"errors"
)

// Tokenizer contains the list of tokens generated by a string, along with the
// current position in that list.
type Tokenizer struct {
	tokens   []*Token
	position int
}

// GetNext returns the next Token in the list.
func (t *Tokenizer) GetNext() (*Token, error) {
	if t.position >= len(tokens) {
		return nil, errors.New("empty tokenizer")
	}

	token := t.tokens[t.position]
	t.position++
	return token, nil
}

// GetAll returns all of the Tokens in the list.
func (t *Tokenizer) GetAll() []*Token {
	return t.tokens
}

// Tokenize creates a Tokenizer from the input string.
// It does this by creating each Kind of Token that it encounters.
// If the function encounters a character for which a valid Token cannot
// be created, the function returns an error.
func Tokenize(input string) (*Tokenizer, error) {
	return nil, nil
}
